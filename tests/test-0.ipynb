{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14d6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to get data set right\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edbfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  # number of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f5280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_digits(num):\n",
    "        # convert numbers into a reversed list of digits, such that index 0 refers to the ones digit\n",
    "        digits = str(num)\n",
    "        if len(digits) < k:\n",
    "            padding = \"0\" * k \n",
    "            digits = padding[:k-len(digits)] + digits\n",
    "        return list(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a42495",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = 2  # which digit to query (0-indexed from the right)\n",
    "max_number = 10**k - 1\n",
    "a1_int = np.random.randint(0, max_number)\n",
    "a2_int = np.random.randint(0, max_number)\n",
    "A_int = a1_int + a2_int\n",
    "\n",
    "a1 = \" \".join(_get_digits(a1_int))\n",
    "a2 = \" \".join(_get_digits(a2_int))\n",
    "A = _get_digits(A_int)[::-1]\n",
    "\n",
    "input_str = f\"A = {a1} + {a2} , A {digit} = ?\"\n",
    "output = int(A[digit])\n",
    "sent = f\"Input: {input_str} Output: {output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b42f89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input: A = 4 4 5 + 4 0 4 , A 2 = ? Output: 8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92266dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# --- fixed vocab (lowercased to be robust if you later add Lowercase normalizer) ---\n",
    "SPECIALS = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "BASIC_TOKENS = [\"Input\", \"Output\", \"A\", \":\", \"=\", \"+\", \",\", \"?\"] + [str(d) for d in range(10)]\n",
    "VOCAB_LIST = SPECIALS + BASIC_TOKENS\n",
    "VOCAB = {tok: i for i, tok in enumerate(VOCAB_LIST)}\n",
    "\n",
    "# ---- tokenizer model ----\n",
    "tk = Tokenizer(WordLevel(vocab=VOCAB, unk_token=\"[UNK]\"))\n",
    "\n",
    "# Use Whitespace pre-tokenizer to split on whitespace\n",
    "tk.pre_tokenizer = Whitespace()\n",
    "\n",
    "# ---- post-processing (BOS/EOS) ----\n",
    "tk.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $0 [EOS]\",\n",
    "    special_tokens=[(\"[BOS]\", VOCAB[\"[BOS]\"]), (\"[EOS]\", VOCAB[\"[EOS]\"])],\n",
    ")\n",
    "\n",
    "# ---- wrap ----\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tk,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030d3b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [4, 7, 6, 8, 12, 16, 16, 9, 20, 20, 20, 10, 6, 13, 8, 11, 5, 7, 15]\n",
      "tokens: ['Input', ':', 'A', '=', '0', '4', '4', '+', '8', '8', '8', ',', 'A', '1', '=', '?', 'Output', ':', '3']\n",
      "decoded: Input : A = 0 4 4 + 8 8 8, A 1 =? Output : 3\n"
     ]
    }
   ],
   "source": [
    "x = \"Input: A = 0 4 4 + 8 8 8 , A 1 = ? Output: 3\"\n",
    "enc = tokenizer(x, add_special_tokens=False, max_length=19, truncation=True)\n",
    "print(\"input_ids:\", enc[\"input_ids\"])\n",
    "print(\"tokens:\", tokenizer.convert_ids_to_tokens(enc[\"input_ids\"]))\n",
    "print(\"decoded:\", tokenizer.decode(enc[\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311cfcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49471443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea251d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class DigitAdditionDataset(Dataset):\n",
    "    def __init__(self, num_samples, k, digit, tokenizer):\n",
    "        self.num_samples = num_samples\n",
    "        self.k = k\n",
    "        self.digit = digit\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        max_number = 10**self.k - 1\n",
    "        a1_int = np.random.randint(0, max_number)\n",
    "        a2_int = np.random.randint(0, max_number)\n",
    "        A_int = a1_int + a2_int\n",
    "        \n",
    "        a1 = \" \".join(_get_digits(a1_int))\n",
    "        a2 = \" \".join(_get_digits(a2_int))\n",
    "        A = _get_digits(A_int)[::-1]\n",
    "        \n",
    "        input_str = f\"A = {a1} + {a2} , A {self.digit} = ?\"\n",
    "        output = int(A[self.digit])\n",
    "        sent = f\"Input: {input_str} Output: {output}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(sent, return_tensors='pt', padding=False, truncation=True)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': output\n",
    "        }\n",
    "\n",
    "class DigitAdditionDatasetAllDigits(Dataset):\n",
    "    def __init__(self, num_samples, k, tokenizer, pair_base: int = 0):\n",
    "        self.num_samples = num_samples\n",
    "        assert num_samples % k == 0, \"num_samples must be multiple of k\"\n",
    "        self.k = k\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pair_base = pair_base # evaluation offset to avoid overlap between train and eval\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples # Each sample generates k examples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which pair and which digit\n",
    "        pair_idx = idx // (self.k) + self.pair_base\n",
    "        digit_pos = idx % (self.k)\n",
    "        \n",
    "        # Use pair_idx as seed for reproducibility within epoch\n",
    "        rng = np.random.RandomState(pair_idx)\n",
    "        max_number = 10**self.k - 1\n",
    "        a1_int = rng.randint(0, max_number)\n",
    "        a2_int = rng.randint(0, max_number)\n",
    "        A_int = a1_int + a2_int\n",
    "        \n",
    "        a1 = \" \".join(_get_digits(a1_int))\n",
    "        a2 = \" \".join(_get_digits(a2_int))\n",
    "        A = _get_digits(A_int)[::-1]\n",
    "        \n",
    "        input_str = f\"A = {a1} + {a2} , A {digit_pos} = ?\"\n",
    "        output = int(A[digit_pos])\n",
    "        sent = f\"Input: {input_str} Output: {output}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(sent, return_tensors='pt', padding=False, add_special_tokens=False)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': output\n",
    "        }\n",
    "# Create dataset and dataloader\n",
    "#train_dataset = DigitAdditionDataset(num_samples=10000, k=k, digit=digit, tokenizer=tokenizer)\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0502d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./addition_tokenizer/tokenizer_config.json',\n",
       " './addition_tokenizer/special_tokens_map.json',\n",
       " './addition_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.save_pretrained(\"./addition_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4051db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 19]) torch.Size([32, 19]) torch.Size([32])\n",
      "torch.Size([32, 19]) torch.Size([32, 19]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DigitAdditionDatasetAllDigits(num_samples=32*3*2000, k=3, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "count = 0\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['label']\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "    count += 1\n",
    "    if count >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2051b14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Input : A = 2 6 5 + 1 2 5, A 2 =? Output : 3',\n",
       " 'Input : A = 9 2 1 + 7 0 3, A 0 =? Output : 4',\n",
       " 'Input : A = 9 2 1 + 7 0 3, A 1 =? Output : 2')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0]), tokenizer.decode(input_ids[1]), tokenizer.decode(input_ids[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7314b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from typing import Optional, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# helper\n",
    "ACT2FN = {\n",
    "    \"relu\": F.relu,\n",
    "    \"gelu\": F.gelu,\n",
    "    \"silu\": F.silu,\n",
    "    \"swish\": F.silu,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttentionConfig:\n",
    "    D: int = 768\n",
    "    layer_idx: Optional[int] = None\n",
    "    n_heads: int = 4\n",
    "    causal: bool = True\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):  # BSD -> BSD\n",
    "    def __init__(self, layer_idx: int, config: AttentionConfig):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.D = config.D\n",
    "        self.n_heads = config.n_heads\n",
    "        assert self.D % self.n_heads == 0\n",
    "        self.head_dim = self.D // self.n_heads\n",
    "        self.Wq = nn.Linear(self.D, self.D, bias=False)\n",
    "        self.Wk = nn.Linear(self.D, self.D, bias=False)\n",
    "        self.Wv = nn.Linear(self.D, self.D, bias=False)\n",
    "        self.causal = config.causal\n",
    "        #self.Wo = nn.Linear(self.D, self.D, bias=False)\n",
    "        #self.Wo.weight.data.zero_()  # initialize to zero for stability\n",
    "        self.W_O = nn.Parameter(torch.zeros(self.n_heads, self.head_dim, self.D))\n",
    "        self.device = config.device\n",
    "        # Hook points\n",
    "        self.hook_attn_pattern = HookPoint()\n",
    "        self.hook_attn_output_per_head = HookPoint()\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, kv_cache: Optional[Any] = None\n",
    "    ) -> torch.Tensor:  # input is [B, S, D]\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        # Make each QKV [B, S, D] --> [B, nh, S, hd]\n",
    "        Q, K, V = self.Wq(x), self.Wk(x), self.Wv(x)  # all [B, S, D]\n",
    "\n",
    "        Q = Q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)  # [B, nh, S, hd]\n",
    "        K = K.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # update kv cache\n",
    "        layer_idx = self.layer_idx\n",
    "        if kv_cache is not None and layer_idx is not None:\n",
    "            # its preallocated, just write to the memory of the cache using state of current_length\n",
    "            kv_cache.update(layer_idx, K, V)\n",
    "            K = kv_cache.keys[layer_idx][:, :, : kv_cache.current_length, :]\n",
    "            V = kv_cache.values[layer_idx][:, :, : kv_cache.current_length, :]\n",
    "\n",
    "        # [B, nh, S, hd] @ [B, nh, hd, S] -> [B, nh, S, S]\n",
    "        scale = torch.sqrt(\n",
    "            torch.tensor(self.head_dim, dtype=Q.dtype, device=self.device)\n",
    "        )\n",
    "        logits = (Q @ K.transpose(-2, -1)) / scale\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(logits), diagonal=1).bool()\n",
    "            logits_masked = logits.masked_fill(mask, float(\"-inf\"))\n",
    "        else:\n",
    "            logits_masked = logits\n",
    "\n",
    "        A = F.softmax(logits_masked, dim=-1)  # [B, nh, S, S]\n",
    "        # Hook attention pattern: [B, nh, S, S]\n",
    "        A = self.hook_attn_pattern(A)\n",
    "\n",
    "        preout = torch.einsum(\n",
    "            \"bnxy,bnyd->bnxd\", A, V\n",
    "        )  # [B, nh, S, hd]\n",
    "\n",
    "        # Rearrange W_O from [D, D] to [nh, hd, D]\n",
    "        #W_O = self.Wo.weight.T.view(self.n_heads, self.head_dim, self.D)\n",
    "        attn_output_per_head = torch.einsum(\n",
    "            \"bnxd,ndh->bnxh\", preout, self.W_O\n",
    "        )  # [B, nh, S, D]\n",
    "        # Reorder to [B, S, nh, D] and hook\n",
    "        attn_output_per_head_seq = attn_output_per_head.transpose(1, 2)\n",
    "        attn_output_per_head_seq = self.hook_attn_output_per_head(attn_output_per_head_seq)\n",
    "        # Sum across heads -> [B, S, D]\n",
    "        attn_out = attn_output_per_head_seq.sum(dim=2)\n",
    "        return attn_out  # [B, S, D]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLPConfig:\n",
    "    D: int\n",
    "    hidden_multiplier: int = 4\n",
    "    act: str = \"gelu\"\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "# most important fact about MLP: it operates on each token independently, ie. D --> D\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: MLPConfig):\n",
    "        super().__init__()\n",
    "        self.D = config.D\n",
    "        self.device = (\n",
    "            config.device\n",
    "            if config.device is not None\n",
    "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.up_proj = nn.Linear(self.D, self.D * config.hidden_multiplier, bias=False)\n",
    "        self.down_proj = nn.Linear(\n",
    "            self.D * config.hidden_multiplier, self.D, bias=False\n",
    "        )\n",
    "        self.down_proj.weight.data.zero_()  # initialize to zero for stability\n",
    "        self.act = ACT2FN[config.act]\n",
    "        # Hook point at MLP mid activation\n",
    "        self.hook_mlp_mid = HookPoint()\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> torch.Tensor:  # BSD -> BSD automatically on last dim\n",
    "        mid = self.act(self.up_proj(x))\n",
    "        mid = self.hook_mlp_mid(mid)  # [B, S, D*mult]\n",
    "        return self.down_proj(mid)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LNConfig:\n",
    "    D: int\n",
    "    eps: float = 1e-9\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "class LN(nn.Module):\n",
    "    def __init__(self, config: LNConfig):\n",
    "        super().__init__()\n",
    "        self.D = config.D\n",
    "        self.eps = config.eps\n",
    "        self.device = (\n",
    "            config.device\n",
    "            if config.device is not None\n",
    "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.mean_scale = nn.Parameter(torch.zeros(self.D))\n",
    "        self.std_scale = nn.Parameter(torch.ones(self.D))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x is [B, S, D]\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # [B, S, 1]\n",
    "        std = (x.var(dim=-1, keepdim=True) + self.eps) ** 0.5  # [B, S, 1]\n",
    "        x_norm = (x - mean) / (std)\n",
    "        return x_norm * self.std_scale + self.mean_scale\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerLayerConfig:\n",
    "    D: int = 768\n",
    "    n_heads: int = 4\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, layer_idx: int, config: TransformerLayerConfig):\n",
    "        super().__init__()\n",
    "        self.D = config.D\n",
    "        self.layer_idx = layer_idx\n",
    "        self.device = (\n",
    "            config.device\n",
    "            if config.device is not None\n",
    "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "        attn_config = AttentionConfig(\n",
    "            D=self.D, n_heads=config.n_heads, device=self.device\n",
    "        )\n",
    "        mlp_config = MLPConfig(D=self.D, device=self.device)\n",
    "        ln_config = LNConfig(D=self.D, device=self.device)\n",
    "\n",
    "        self.attn = Attention(self.layer_idx, attn_config)\n",
    "        self.mlp = MLP(mlp_config)\n",
    "        self.ln1 = LN(ln_config)\n",
    "        self.ln2 = LN(ln_config)\n",
    "        # Residual stream hook points\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, kv_cache: Optional[Any] = None, return_attn: bool = False\n",
    "    ) -> torch.Tensor:  # x is BSD\n",
    "        x = self.hook_resid_pre(x)\n",
    "        ln1_out = self.ln1(x)\n",
    "        attn_out = self.attn(ln1_out, kv_cache=kv_cache)\n",
    "        x = x + attn_out\n",
    "        x = self.hook_resid_mid(x)\n",
    "        ln2_out = self.ln2(x)\n",
    "        mlp_out = self.mlp(ln2_out)\n",
    "        x = x + mlp_out\n",
    "        x = self.hook_resid_post(x)\n",
    "        if return_attn:\n",
    "            return x, attn_out\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PositionalEmbeddingConfig:\n",
    "    max_seq_len: int\n",
    "    D: int\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config: PositionalEmbeddingConfig):\n",
    "        super().__init__()\n",
    "        self.device = (\n",
    "            config.device\n",
    "            if config.device is not None\n",
    "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(config.max_seq_len, config.D))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x is [B, S, D]\n",
    "        B, S, D = x.shape\n",
    "        return x + self.pos_embedding[:S]  # Broadcasting handles batch dimension\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingLayerConfig:\n",
    "    vocab_size: int\n",
    "    D: int\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config: EmbeddingLayerConfig):\n",
    "        super().__init__()\n",
    "        self.device = (\n",
    "            config.device\n",
    "            if config.device is not None\n",
    "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.embedding = nn.Parameter(torch.randn(config.vocab_size, config.D))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding[x]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnembeddingLayerConfig:\n",
    "    vocab_size: int\n",
    "    D: int\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "class UnembeddingLayer(nn.Module):\n",
    "    def __init__(self, config: UnembeddingLayerConfig):\n",
    "        super().__init__()\n",
    "        self.device = (\n",
    "            config.device\n",
    "            if config.device is not None\n",
    "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.V = config.vocab_size\n",
    "        self.unembedding = nn.Linear(config.D, self.V, bias=False)\n",
    "        self.unembedding.weight.data.zero_() # initialize to zero for stability\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x is [B, S, D]\n",
    "        return self.unembedding(x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    hidden_dim: int = 768\n",
    "    depth: int = 2\n",
    "    n_heads: int = 4\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 128\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.depth = config.depth\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        emb_config = EmbeddingLayerConfig(\n",
    "            vocab_size=config.vocab_size, D=config.hidden_dim, device=config.device\n",
    "        )\n",
    "        pos_emb_config = PositionalEmbeddingConfig(\n",
    "            max_seq_len=config.max_seq_len, D=config.hidden_dim, device=config.device\n",
    "        )\n",
    "        unemb_config = UnembeddingLayerConfig(\n",
    "            vocab_size=config.vocab_size, D=config.hidden_dim, device=config.device\n",
    "        )\n",
    "\n",
    "        self.emb = EmbeddingLayer(emb_config)\n",
    "        self.pos_emb = PositionalEmbedding(pos_emb_config)\n",
    "\n",
    "        self.ln_final = LN(LNConfig(D=config.hidden_dim, device=config.device))\n",
    "        self.unemb = UnembeddingLayer(unemb_config)\n",
    "\n",
    "        layer_config = TransformerLayerConfig(\n",
    "            D=config.hidden_dim, n_heads=config.n_heads, device=config.device\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(idx, layer_config) for idx in range(config.depth)]\n",
    "        )\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.attn.layer_idx = i\n",
    "\n",
    "        self.device = config.device\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, kv_cache: Optional[Any] = None, return_attn: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.emb(x)\n",
    "        if kv_cache is not None:\n",
    "            # When decoding, only add positional embeddings for the new tokens.\n",
    "            pos_offset = kv_cache.current_length\n",
    "            pos_emb = self.pos_emb.pos_embedding[\n",
    "                pos_offset : pos_offset + x.size(1)\n",
    "            ].unsqueeze(0)\n",
    "            x = x + pos_emb\n",
    "        else:\n",
    "            x = self.pos_emb(x)\n",
    "\n",
    "        all_attn = []\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            if return_attn:\n",
    "                x, attn = layer(x, kv_cache=kv_cache, return_attn=True)\n",
    "                all_attn.append(attn)\n",
    "            else:\n",
    "                x = layer(x, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.unemb(x)\n",
    "        if return_attn:\n",
    "            return logits, torch.stack(all_attn, dim=0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11cfb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(\n",
    "    hidden_dim=128*6,\n",
    "    depth=2,\n",
    "    n_heads=6,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=19,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "model = Transformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57864a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 19, 22])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "logits.shape  # should be [B, S, Vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96926299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 19])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0841ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 2, 6, 0, 1, 5, 4, 1, 5, 3, 6, 9, 3, 1, 2, 9, 1, 9, 4, 6, 8, 3, 7,\n",
       "        5, 7, 2, 2, 9, 2, 8, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4ce5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logits for the last token position\n",
    "last_token_logits = logits[:, -2, :]  # [B, vocab_size]\n",
    "\n",
    "loss = F.cross_entropy(last_token_logits, input_ids[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c93c6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "logits = model(input_ids)\n",
    "loss = F.cross_entropy(logits[:, :-1, :].contiguous().view(-1, logits.size(-1)), \n",
    "                       input_ids[:, 1:].contiguous().view(-1))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
